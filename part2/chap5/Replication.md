## Synchronous vs Asynchronous Replication

synchronous replication의 장점은 follower가 leader와 항상 일치하는 데이터를 가진다는 점이다. 하지만 write작업시 모든 follower와 동기화될때까지 블락킹된다는 단점이 있다. 일부 follower에만 synchronous하게 동기화하는 것을 semi-synchronous라고 한다.

fully asynchronous replication의 경우 write가 durable하지 않다. 하지만 leader가 계속적으로 write작업을 처리할수 있다는 장점이 있다.



## 노드 과부하 핸들링

Follower에 장애가 생기는 경우, 로그파일을 읽어서 장애가 난 이후부터의 데이터를 leader와 동기화할수 있다.

Leader에 장애가 생기는 경우 아래와 같은 세가지 단계로 복구하는데 이러한 프로세스를 failover라 한다.

- 리더의 장애 감지하기
- 새로운 리더 선출
- 시스템이 새로운 리더에게 쓰기 작업하도록 설정하기

failover 과정에서 여러 오류가 발생할 수 있다. 먼저 비동기 복제방식의 경우, 새로운 리더는 이전 리더에게 요청된 모든 쓰기작업을 수신하지 않을수 있는데, 이러한 경우 이전 리더의 상태와 일관성이 깨지게 된다. 보통 이러한 경우 이전리더에 있던 반영되지 않았던 쓰기작업은 버려지게 되는데, 이는 클라이언트의 durability 기대에 어긋난다.

또한 외부 시스템에서 데이터에 의존하는 경우에, 새로운 리더와 이전 리더와의 데이터 차이로 인해서 데이터 비일관성이 발생할 수 있다.

리더가 두개이상일 경우, 안정적으로 두 리더중 하나를 제거하는것도 필요하다.



## 로그 복제 구현

### Statement-based replication

리더에서 follower로 sql statement를 그대로 전달해서 데이터를 복제하는 방법이다.

다음과 같은 문제점이 있다.

- `NOW()`나 `RAND()`와 같이 nondeterministic한 함수를 호출하는 경우 쿼리결과가 달라진다.
- 기존에 존재하고 있는 데이터에 의존하거나 자동으로 값이 증가하는 칼럼이 있는 경우도, 주의해야 한다.

nondeterministic함수들을 값으로 치환한뒤 복제하는 것도 가능은 하지만 여러 엣지 케이스들이 많기 때문에 보통 다른 방법이 사용됨.



### Write-ahead log (WAL) shipping

디비에대한 write작업을 바이트로 직렬화하여 WAL에 기록하고, 이를 follower에 전달하여 복제하는 방식이다.

이 방식의 문제점은 WAL과 스토리지 엔진이 강하게 커플링이 되어있다는 점이다. 그래서 스토리지 엔진 버전이 일치하지 않으면 follower에서 해당 WAL을 제대로 읽을수 없다는 문제가 있다. 이럴경우, 스토리지 엔진을 업데이트해야할수도 있는데 이경우 디비에 downtime이 발생한다.



### Logical(row-based) log replication

스토리지 엔진과 결합되어 있지않은 포맷으로 replication log를 표현하는 방법도 있다. 이러한 방식을 logical log라고 부른다. 스토리지 엔진과 직접적으로 커플링이 되어있지 않아서, backward compatible을 지원할 가능성이 높다. 또한 외부 어플리케이션에서 이 로그를 읽어들이기도 쉽다.



### Triggered-based replication

트리거는 데이터가 변경되었을때, 자동으로 특정 커스텀 코드를 실행하도록 해주는 역할을 한다. 트리거로 데이터 변경을 감지하고 변경로그를 별도의 테이블에 기록한뒤 외부 어플리케이션에서 이에 접근하는 방법으로 replication할수 있다. 트리거는 버그와 한계점이 있고 오버헤드를 유발한다고 한다.



### read-after-write consistency를 구현하는 방법

- 유저가 수정한 데이터는 무조건 리더로 부터 읽기
- 유저가 수정한 시간을 기록하여 그 시간을 기준으로 특정시간이 지난경우 리더와 싱크를 맞추는 방법
- 리플리카들이 여러 데이터 센터에 분산되있는 경우, 리더가 속해있는 데이터 센터를 찾아야 하는 번거로움이 존재한다. 또한 네트워크 접속 방식에 따라서 다른 데이터 센터로 라우팅 되는 경우가 있어서 결국 다른 리플리카와 연결이 되는 경우가 생길수 있으므로 항상 같은 데이터 센터로 라우팅되도록 해주어야 한다. 그렇지 않으면 연결상태에 따라 최신 데이터가 보일수도 있고 안보일수도 있다.



### Monotonic Reads

Monotonic Reads란, 데이터를 읽을때 이전 버전의 데이터를 읽을수 있지만, 그 다음 데이터를 읽을때는 이미 읽었던 데이터 보다 이전 데이터는 읽지 않을것을 보장하는것을 말한다. 이를 구현하는 한가지 방법은 각각의 유저가 같은 리플리카로 부터 데이터를 읽을 수 있도록 하는 방법이 있다.

consistent prefix란, 만약 일련의 쓰기작업들이 특정 순서로 발생하면 해당 데이터를 읽는 사람은 항상 쓰기 작업이 적용된 순서대로 데이터를 읽을수 있어야하는것을 말한다.

한가지 해결방법은 인과적으로 연관된 쓰기작업을 같은 파티션에 적용하는 방법이 있는데, 이는 비효율적이라는 단점이 있다. 그리고 이러한 인과 관계를 명시적으로 추적할수 있는 알고리즘도 존재한다.





# Multi-leader Replication



leader-based 어플리케이션은 한번에 하나의 노드만 write를 처리할수 있다는 단점이 있었다. Multi-leader replication은 이 단점을 보완한다.

다중 리더 복제가 사용되는 여러가지 도메인을 소개하면 아래와 같다.

- Multi-datacenter operation
  - 이 경우아래와 같은 장점이 있다.
    - Performance 증가 : 모든 쓰기 작업은 가까운 로컬 데이터 센터에서 처리될 수 있어서, 유저입장에서 퍼포먼스가 증가했다고 느낄수 있다.
    - 데이터 센터 아웃테이지를 어느정도 감수가능 : 각각의 데이터 센터는 독립적으로 동작할수 있고, 어느 한 데이터 센터가 실패했다가 다시 돌아오면 그때 서로를 싱크할 수 있다.
    - 네트워크 문제를 감수가능 : 비동기적으로 쓰기작업이 처리될수 있다는 장점때문에 일시적인 네트워크 문제가 쓰기작업을 중단시키진 않는다.
  - 대부분의 디비에서는 이러한 기능을 기본적으로 제공하지는 않고 외부 툴들을 통해서 제공한다.
  - 가장 큰 단점은 서로 다른 데이터 센터에 데이터가 동시적으로 수정될수 있는데, 이때 이 컨플릭트가 발생할수 있다는 점이다.
  - 다중 리더 복제의 단점는 미묘한 configuration pitfall, 예상할수 없는 동작으로 인해서 가능하면 종종 피해야 한다고 여겨진다.
- Clients with offline operation
  - 대표적으로 달력 앱과같이 오프라인일때도 쓰기 작업을 처리할수 있어야 하는 것을 예로 들수 있다.
- Collaborative editing
  - 대표적으로 구글독스에서 동시적으로 여러 유저가 문서를 수정하는것을 예로 들수 있다.

다중 리더 복제에서 가장 큰 문제는 바로 쓰기 컨플릭이다. 여기서 동기적으로 컨플릭을 감지하는 방법과 비동기적으로 컨플릭을 감지하는 방법이 있는데, 동기적으로 컨플릭을 감지하는 경우 다중 리더복제가 갖는 장점(각각의 리더가 독립적으로 쓰기작업을 처리가능)을 잃어버린다는 문제가 있다.

가장 간단한 전략은 쓰기 컨플릭을 피하는 것인데, 예를들어 사용자별로 고정된 데이터 센터로 라우팅하는 방법이 있다. 하지만 다른 데이터 센터로 맵핑하고자하는 경우(이전에 맵핑된 데이터센터가 실패한경우)에는 컨플릭을 해결해야한다.

그 다음전략은 일관된 상태로 수렴시키는 것이다. 다양한 방법들을 소개하면 아래와같다.

- 각 쓰기 작업에 대해서 unique id를 부여하고, 컨플릭이 발생한 쓰기작업의 unique id중 가장 높은 id를 winner로 설정할수 있다. 이러한 방식을 last write wins(LWW)라고 한다. 이 방식은 데이터 손실이 발생할 수 있다.
- 각각의 리플리카에 unique id를 부여하고 높은 번호를 가진 리플리카로 부터 유래한 쓰기를 낮은 번호를 가진 리플리카로 부터 유래한 쓰기보다 우선하는 방법이다. 역시 데이터 손실이 발생할 수 있다.
- 컨플릭이 발생하는 key에 대한 값들을 머지하는 방법이다.
- 모든 정보를 유지하는 명시적인 데이터 구조로 컨플릭을 기록해두고, 어플리케이션에서 후에 이러한 컨플릭을 해결한다.

다른 전략은 커스텀 컨플릭 해결 로직을 구현하는 것이다. 대부분의 다중 리더 복제 툴들은 커스텀한 컨플릭 해결 로직을 작성할수 있도록 해준다.

다중 리더 복제에는 여러 토폴로지가 있는데 대표적으로 circular, star, all-to-all topology가 있다.

all-to-all topology가 가장 일반적이며, 리플리케이션의 무한 루프를 방지하기 위해 각각의 쓰기작업에는 이미 지나간 노드들의 식별자들로 태깅되어 있다. all-to-all 토폴로지의 경우 다른 두 토폴로지에 비해 single point of failure를 피하는 면에서는 장점이있다.

반면 all-to-all 토폴로지는 쓰기 순서에 따라서 causality 문제가 발생할 수 있다. 단순히 타임스탬프를 모든 쓰기에 추가하는 것으로는 충분하지 않다고 한다.



# Leaderless Replication

leaderless 복제는 Riak, Cassandra, Voldemort와 같은 오픈소스 데이터 스토어에서 주로 사용되어 왔고, 이런종류의 디비를 dynamo에서 영감을 받았다고 해서 dynamo-style 디비라고 한다.

리더리스 복제에서는 모든 리플리카 노드가 쓰기작업을 수락할수 있는데, 이때 특정 리플리카 노드가 죽었을수 있기 때문에 다시 살아났을때 놓쳤던 쓰기작업을 따라잡는 작업이 필요하다.

다음의 두가지 메커니즘이 주로 사용된다.

- Read repair
  - 클라이언트가 여러 리플리카로부터 데이터를 읽어서 이전 버전 데이터를 가진 리플리카에 최신 데이터로 업데이트 한다.
  - 자주 읽히는 데이터가 아니라면 몇몇 리플리카에 데이터가 복사되지 않아 durability가 감소할수 있다는 문제가 있다.
- Anti-entropy process
  - 일부 데이터 스토어는 리플리카들의 데이터들을 서로 비교하여 손실된 데이터를 감지해내고 복사하는 백그라운드 프로세스를 갖고 있다. 이 방식은 상당한 딜레이를 유발할수 있고, 쓰기 작업의 순서가 보장되지 않을수 있다.

여기서 리플리카중 몇개의 노드가 다운될수 있는지 판단하기 위해서 아래의 공식을 사용할수 있다.

>w + r > n, w : 쓰기작업이 성공해야 하는 리플리카 수, r : 읽어야하는 리플리카수, n : 총 노드 수

위의 공식을 만족하는 읽기/쓰기를 quorum reads and writes라고 부른다.

위 공식에서 반드시 w,r이 n/2를 넘을 필요는 없다. 쓰기 대상인 노드와 읽기 대상인 노드가 겹치기만 하면된다. 또한 `w+r<=n` 조건을 만족하는 경우는, 높은 확률로 이전 버전 데이터를 읽게 되지만 HA와 low latency를 제공한다는 장점이 있다.

무엇보다 `w+r > n` 조건을 만족하더라도 이전 버전 데이터를 반환하는 케이스가 있다.

- sloppy quorum이 사용된 경우, 쓰기 대상인 노드와 읽기 대상인 노드가 겹치지 않는 경우가 생긴다.
- 컨플릭이 발생한 값을 타임스탬프값을 사용해서 머지하는 경우 clock skew때문에 데이터 손실이 발생할수 있다.
- 쓰기 작업이 읽기작업과 동시에 발생하는 경우, 쓰기작업은 일부 리플리카에 반영이 된 경우 리플리카에서 이전 값을 반환할지 새로운 값을 반환할지 확신할수 없다.
- 쓰기 작업이 일부 노드에서만 성공하고, 이 값들이 제대로 롤백되지 않은 경우, 리플리카로 부터 읽은 값들이 최신 값을 리턴한다는 보장이 없다.
- 위의 문제가 없다 하더라도 타이밍이 안맞아서 문제가 되는 상황도 있다.

따라서 꼭 quorum read/write를 전적으로 신뢰해서는 안된다.

더 강한 보장을 원하면 트랜잭션과 consensus가 필요하다.

그리고 staleness를 측정하는데 있어서, 리더 기반 복제보다 어려운 점이 있다. 리더 기반 복제는 follower의 포지션과 리더의 포지션의 차이로 lag를 구할수 있지만 리더리스 복제의 경우는 데이터가 쓰여지는 고정된 순서가 없으며, read repair만 사용시 데이터가 얼마나 오래된 데이터로 유지할지에 대한 제한을 줄수가 없다. 디비의 staleness를 측정하기 위한 몇가지 연구가 있긴하지만 흔하게 사용되고 있지 않고 있다.

이외에도 `Sloppy Quorum`이라는 것이 있는데, 이는 w와 r개의 성공적인 응답이 필요하지만 반드시 지정된 n 노드들일 필요없는 경우를 말한다. 만약 네트워크 방해가 발생할경우, 지정된 n노드이외의 노드에서 데이터를 받았다가 네트워크 상태가 돌아오면 해당 데이터를 n노드들로 전송된다. 이를 `hinted handoff`라 한다. sloppy quorum은 특히 write availability를 증가시키는데 유용하다. 그러나 이말은 w+r > n 조건을 만족한다고 하더라도 가장 최신의 데이터를 읽을수 없다는 것을 의미한다.(최신 데이터가 n노드 이외의 노드에 일시적으로 기록될수 있으므로) 따라서 hinted handoff가 완료된경우에만 최신데이터를 읽는다고 확신할수 있다.

리더리스 복제는 다중 리더 복제처럼 multi-datacenter operation에 적합하다. Cassandra와 Voldemort는 normal leaderless model에 기반하며, 이 모델은 쓰기작업이 데이터 센터상관없이 모든 리플리카로 전송되고, 클라이언트는 로컬 데이터센터에 있는 노드들의 quorum으로 부터 ack를 받는다. 이는 데이터 센터간 전송으로 인한 딜레이에 영향받지 않기 위함이다. 데이터 센터간 통신은 보통 비동기적으로 일어난다.

추가적으로 쓰기 컨플릭을 다루는 방법을 알아보면 먼저 Last write wins(LWW)가 있다.

LWW는 eventual convergence를 보장하는 방법이긴 하지만 durability를 보장하지는 않는다. 그리고 데이터 손실이 없어야 된다면 사용하지 않는것이 좋다. LWW를 쉽게 사용하는 방법은 키를 Unique하게 만드는 것이다. 이렇게 하면 동시 업데이트 문제를 예방할수 있다. 대표적으로 Cassandra가 UUID를 key로 사용하는 것을 추천하고 있다.

